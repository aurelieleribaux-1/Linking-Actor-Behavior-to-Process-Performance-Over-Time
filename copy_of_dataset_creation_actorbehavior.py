# -*- coding: utf-8 -*-
"""Copy of Dataset_creation_actorbehavior.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G3B7E2nCL15s9CLUeGIp22iXlHjabfXm
"""

!pip install pm4py pandas numpy

import pm4py
import pandas as pd
from google.colab import files

# Upload de dataset handmatig
uploaded = files.upload()

# Lees de event log
log = pm4py.read_xes("BPI Challenge 2017.xes.gz")
df = pm4py.convert_to_dataframe(log)

df.head()

import pm4py
import pandas as pd
from google.colab import files

# Upload de dataset handmatig
uploaded = files.upload()

# Lees de event log
log = pm4py.read_xes("BPI_Challenge_2012.xes.gz")
df = pm4py.convert_to_dataframe(log)

df.head()

import pm4py
import pandas as pd
from google.colab import files

# Upload de dataset handmatig
uploaded = files.upload()

# Lees de event log
log = pm4py.read_xes("BPI_Challenge_2019.xes")
df = pm4py.convert_to_dataframe(log)

df.head()

print(df.columns)

import pm4py
import pandas as pd
from google.colab import files

# Upload de dataset handmatig
uploaded = files.upload()

# Lees de event log
log = pm4py.read_xes("Hospital_log.xes.gz")
df = pm4py.convert_to_dataframe(log)

df.head(100)

print(df.columns())

"""#extra hospital log"""

# Zorg dat je pandas hebt geÃ¯mporteerd
import pandas as pd

# Zorg dat je logbestand correct is ingelezen, bijv.:
# df = pm4py.convert_to_dataframe(pm4py.read_xes("Hospital_log.xes.gz"))

# Unieke activiteiten ophalen uit de kolom 'concept:name'
unieke_activiteiten = df['concept:name'].dropna().unique()

# Sorteer ze alfabetisch voor overzicht
unieke_activiteiten = sorted(unieke_activiteiten)

# Print de lijst
print("Unieke activiteiten in 'concept:name':")
for activiteit in unieke_activiteiten:
    print("-", activiteit)

import gzip
import xml.etree.ElementTree as ET
import pandas as pd

# Bestand openen
with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

# Namespace
XES_NS = {'xes': 'http://www.xes-standard.org/'}

# Verzamelen van rows
rows = []
for trace in root.findall('xes:trace', XES_NS):
    case_id = None
    for attr in trace.findall('xes:string', XES_NS):
        if attr.attrib['key'] == 'concept:name':
            case_id = attr.attrib['value']
            break
    for event in trace.findall('xes:event', XES_NS):
        row = {'case_id': case_id}
        for attr in event:
            key = attr.attrib.get('key')
            value = attr.attrib.get('value')
            if key and value:
                row[key] = value
        rows.append(row)
    if len(rows) > 50:  # Beperk tot 50 events
        break

# DataFrame aanmaken en tonen
df = pd.DataFrame(rows)
print(df.head(50))  # Of gebruik display(df) in Jupyter

import gzip
import xml.etree.ElementTree as ET

# XES-bestand openen
with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

# Namespace voor XES
XES_NS = {'xes': 'http://www.xes-standard.org/'}

# Verzamel unieke kolomnamen
event_attributes = set()
trace_attributes = set()

for trace in root.findall('xes:trace', XES_NS):
    for attr in trace:
        if "key" in attr.attrib:
            trace_attributes.add(attr.attrib["key"])
    for event in trace.findall('xes:event', XES_NS):
        for attr in event:
            if "key" in attr.attrib:
                event_attributes.add(attr.attrib["key"])

# Resultaten tonen
print("ğŸŸ¨ Trace-level attributen (bijv. case info):")
for key in sorted(trace_attributes):
    print(" -", key)

print("\nğŸŸ¦ Event-level attributen (per activiteit):")
for key in sorted(event_attributes):
    print(" -", key)

import gzip
import xml.etree.ElementTree as ET

# Bestand openen (pas naam aan indien anders)
with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

# XES namespace
XES_NS = {'xes': 'http://www.xes-standard.org/'}

# Verzamel alle unieke kolomnamen
all_keys = set()

for trace in root.findall('xes:trace', XES_NS):
    for attr in trace:
        key = attr.attrib.get("key")
        if key:
            all_keys.add(key)
    for event in trace.findall('xes:event', XES_NS):
        for attr in event:
            key = attr.attrib.get("key")
            if key:
                all_keys.add(key)

# Sorteer en toon ze allemaal
print("ğŸ§¾ Alle kolomnamen (keys) in de XES-log:")
for key in sorted(all_keys):
    print("-", key)

import gzip
import xml.etree.ElementTree as ET

# Bestand openen
with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

XES_NS = {'xes': 'http://www.xes-standard.org/'}

# EÃ©n volledige rij uitlezen
row = {}
for trace in root.findall('xes:trace', XES_NS):
    for attr in trace:
        if "key" in attr.attrib and "value" in attr.attrib:
            row[f"trace:{attr.attrib['key']}"] = attr.attrib["value"]
    for event in trace.findall('xes:event', XES_NS):
        for attr in event:
            if "key" in attr.attrib and "value" in attr.attrib:
                row[attr.attrib["key"]] = attr.attrib["value"]
        break  # Alleen eerste event
    break  # Alleen eerste trace

# ğŸ‘‰ Kolomnamen op volgorde tonen
print("ğŸ§¾ Kolomnamen van links naar rechts:")
print(list(row.keys()))

# Installeer Excel-ondersteuning (alleen nodig als dit je eerste keer is)
!pip install openpyxl

# ğŸ“¥ Upload de XES.GZ file
from google.colab import files
uploaded = files.upload()  # selecteer hier je 'Hospital_log.xes.gz'

# ğŸ§± XML + gzip uitlezen
import gzip
import xml.etree.ElementTree as ET
import pandas as pd

# Bestand openen
with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

# Namespace
XES_NS = {'xes': 'http://www.xes-standard.org/'}

# Data extraheren
rows = []
for trace in root.findall('xes:trace', XES_NS):
    trace_data = {}
    for attr in trace:
        if "key" in attr.attrib and "value" in attr.attrib:
            trace_data[f"trace:{attr.attrib['key']}"] = attr.attrib["value"]
    for event in trace.findall('xes:event', XES_NS):
        row = trace_data.copy()
        for attr in event:
            if "key" in attr.attrib and "value" in attr.attrib:
                row[attr.attrib["key"]] = attr.attrib["value"]
        rows.append(row)

# ğŸ“Š Data opslaan naar Excel
df = pd.DataFrame(rows)
output_filename = "Hospital_log_full.xlsx"
df.to_excel(output_filename, index=False)

# ğŸ“¤ Download Excel-bestand naar jouw computer
files.download(output_filename)

# Vereiste imports
import gzip
import xml.etree.ElementTree as ET
import pandas as pd

# Bestand uploaden via Colab
from google.colab import files
uploaded = files.upload()  # Upload hier je Hospital_log.xes.gz

# Bestand lezen
with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

# XES namespace
XES_NS = {'xes': 'http://www.xes-standard.org/'}

# Events extraheren
rows = []
for trace in root.findall('xes:trace', XES_NS):
    case_id = None
    for attr in trace.findall('xes:string', XES_NS):
        if attr.attrib['key'] == 'concept:name':
            case_id = attr.attrib['value']
            break
    for event in trace.findall('xes:event', XES_NS):
        row = {'case_id': case_id}
        for attr in event:
            key = attr.attrib.get('key')
            value = attr.attrib.get('value')
            if key and value:
                row[key] = value
        rows.append(row)

# DataFrame maken
df = pd.DataFrame(rows)

# Excel-bestand maken
excel_filename = "Hospital_log.xlsx"
df.to_excel(excel_filename, index=False)

# Downloadlink aanbieden
files.download(excel_filename)

"""##Functie BPIC 2017"""

# Zorg dat timestamp een datetime is
df["time:timestamp"] = pd.to_datetime(df["time:timestamp"])

# Filter alleen Offer-gerelateerde events
offers = df[df["EventOrigin"] == "Offer"].copy()

# Sorteer op tijd, neem het laatste Offer-event per case
offers_sorted = offers.sort_values("time:timestamp")
last_offer = offers_sorted.groupby("case:concept:name").tail(1)

# Map laatste Offer-event naar uitkomst
outcome_map = {
    "O_Accepted": "Accepted",
    "O_Refused": "Refused",
    "O_Cancelled": "Canceled"
}
last_offer["case:outcome"] = last_offer["concept:name"].map(outcome_map)

# Merge 'case:outcome' terug naar hoofddataset
df = df.merge(
    last_offer[["case:concept:name", "case:outcome"]],
    on="case:concept:name",
    how="left"
)

# Verwijder cases zonder geldige outcome
df = df[df["case:outcome"].notna()].copy()

# Voeg binaire kolommen toe
df["case_accepted"] = (df["case:outcome"] == "Accepted").astype(int)
df["case_refused"] = (df["case:outcome"] == "Refused").astype(int)
df["case_canceled"] = (df["case:outcome"] == "Canceled").astype(int)

print(df["case:outcome"].value_counts())
print(df[["case_accepted", "case_refused", "case_canceled"]].sum())

if "Accepted" in df.columns:
    num_missing = df["Accepted"].isna().sum()
    total_rows = len(df)
    print(f"âœ… 'Accepted' column found.")
    print(f"ğŸ” Missing values: {num_missing}/{total_rows} ({(num_missing/total_rows)*100:.2f}%)")
    print(df["Accepted"].value_counts(dropna=False))
else:
    print("âŒ 'Accepted' column not found in the event log.")

# Selecteer relevante kolommen
df = df[["case:concept:name", "concept:name", "org:resource", "time:timestamp", "Accepted", "EventOrigin", "case:outcome","case_accepted", "case_canceled", "case_refused"]]
df.columns = ["case_id", "event", "actor", "timestamp", "Accepted", "EventOrigin", "case:outcome","case_accepted", "case_canceled", "case_refused"]
df["timestamp"] = pd.to_datetime(df["timestamp"])

# Opslaan als CSV
df.to_csv("BPIC17.csv", index=False)

!pip install polars tqdm

def classify_actor_behavior(df):
    """Bepaalt de actor-gedrag classificatie"""
    classified_edges = []
    actor_events = {}  # Snelle dictionary lookup

    df = df.sort("case_id", "timestamp")  # Sorteer case-logs op tijd

    for i in range(len(df) - 1):
        e_i, e_j = df[i], df[i + 1]

        # Controleer of de twee events bij dezelfde case horen
        if e_i["case_id"] != e_j["case_id"]:
            continue  # Als ze niet in dezelfde case zitten, gaan we verder

        # Actoren en tijdstippen ophalen
        r_i, r_j = e_i["actor"], e_j["actor"]
        t_i, t_j = e_i["timestamp"], e_j["timestamp"]
        event_i , event_j= e_i["event"], e_j["event"]
        delta_t = (t_j - t_i).total_seconds()  # Tijdverschil in seconden

        # Zelfde actor â†’ Continuation of Interruption
        if r_i == r_j:
            concurrent_cases = df[ (df["actor"] == r_i) & (df["timestamp"] > t_i) & (df["timestamp"] < t_j)] #event uit de dataframe waarvoor de actor r_i (=r_j is) en het tijdstip tussen ti en tj ligt
#Beslissing:
#Als concurrent_cases NIET leeg is â†’ Interruption (I).
#Als concurrent_cases wel leeg is â†’ Continuation (C).
            behavior = "I" if concurrent_cases else "C"
        else:
            # Andere actor â†’ Handover
            concurrent_cases = df[ (df["actor"] == r_j) & (df["timestamp"] > t_i) & (df["timestamp"] < t_j)] #event uit de dataframe waarvoor de actor r_j en het tijdstip tussen ti en tj ligt

            if not concurrent_cases:
                behavior = "HI"   # Handover Idle â†’ geen activiteit
            else:
                concurrent_event = df[ (df["event"] == event_j) & (df["actor"] == r_j) & (df["timestamp"] < t_i) & (df["timestamp"] > t_j)] #idem event uit de dataframe waarvoor het tijdstip NIET tussen ti en tj ligt
                behavior = "HP" if not concurrent_event  else "HD"
                # Handover Prioritized â†’ actor begon aan de case en niet aan een andere case waar dezelfde event ook kon uitgevoerd worden
                # Handover Deprioritized â†’ actor startte en beÃ«indigde een nieuwe case

        # Voeg resultaat toe
        classified_edges.append((e_i["case_id"], e_i["event"], t_i, e_j["event"], t_j, r_i, r_j, behavior, delta_t, e_i["Accepted"], e_j["Accepted"]))


        # Voeg actor events toe, maar bewaar alleen relevante cases
        actor_events.setdefault(r_i, []).append({"timestamp_i": t_i, "case_id": e_i["case_id"]})
        actor_events.setdefault(r_j, []).append({"timestamp_j": t_j, "case_id": e_j["case_id"]})

    return classified_edges

import polars as pl
import pandas as pd
import multiprocessing as mp
from tqdm import tqdm

# ğŸ“‚ Configuratie
input_file = "BPIC17.csv"
output_file = "actor_behavior_results.csv"
chunk_size = 100000  # Aangepast voor snelheid en RAM-gebruik
num_workers = max(mp.cpu_count() - 1, 1)  # Gebruik alle CPU-kernen behalve 1

# ğŸ“ Header schrijven naar output-bestand
with open(output_file, "w") as f:
    f.write("case_id,event_i,timestamp_i,event_j,timestamp_j,actor_i,actor_j,behavior,delta_t,accepted_i,accepted_j,case_outcome,case_accepted,case_refused,case_canceled\n")

def classify_actor_behavior(df_dict):
    """ğŸ” Supersnelle actor-gedrag classificatie met Polars"""
    classified_edges = []
    actor_events = {}  # Snelle dictionary lookup
    df = pl.DataFrame(df_dict)  # Zet om naar Polars DataFrame
    df = df.sort(["case_id", "timestamp"])  # Sorteer op case en tijd

    for i in range(len(df) - 1):
        e_i, e_j = df.row(i, named=True), df.row(i + 1, named=True)  # Gebruik dictionaries

        if e_i["case_id"] != e_j["case_id"]:  # Alleen events binnen dezelfde case analyseren
            continue

        r_i, r_j = e_i["actor"], e_j["actor"]
        t_i, t_j = e_i["timestamp"], e_j["timestamp"]
        delta_t = (t_j - t_i).total_seconds()

        if r_i == r_j:
            concurrent_cases = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_i, []))
            behavior = "I" if concurrent_cases else "C"
        else:
            concurrent_cases = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_j, []))
            task_instances = any(e["case_id"] != e_j["case_id"] and t_i < e["timestamp"] < t_j for e in actor_events.get(r_j, []))
            behavior = "HI" if not concurrent_cases else ("HP" if not task_instances else "HD")

        classified_edges.append((
    e_i["case_id"], e_i["event"], t_i, e_j["event"], t_j,
    r_i, r_j, behavior, delta_t,
    e_i.get("Accepted"), e_j.get("Accepted"),
    e_i.get("case:outcome"),
    e_i.get("case_accepted"), e_i.get("case_refused"), e_i.get("case_canceled")
))


        # Actor events opslaan
        actor_events.setdefault(r_i, []).append({"timestamp": t_i, "case_id": e_i["case_id"]})
        actor_events.setdefault(r_j, []).append({"timestamp": t_j, "case_id": e_j["case_id"]})

    return classified_edges

def process_chunk(chunk):
    """âš¡ Chunk verwerken in Polars en resultaat opslaan"""
    chunk = pl.DataFrame(chunk)  # Zet naar Polars voor snelheid
    chunk = chunk.with_columns(pl.col("timestamp").str.to_datetime("%Y-%m-%d %H:%M:%S.%f", strict=False))
    results = classify_actor_behavior(chunk.to_dict(as_series=False))  # Fix voor lijst naar dict-conversie

    # Direct opslaan om geheugenverbruik te minimaliseren
    with open(output_file, "a") as f:
        for row in results:
            f.write(",".join(map(str, row)) + "\n")

    return len(results)

# ğŸš€ Parallel chunks lezen en verwerken
with mp.Pool(num_workers) as pool:
    total_rows = sum(1 for _ in open(input_file)) - 1  # Aantal rijen tellen (exclusief header)

    with tqdm(total=total_rows, unit="rows") as pbar:
        for _ in pool.imap_unordered(process_chunk, pd.read_csv(input_file, chunksize=chunk_size)):
            pbar.update(chunk_size)

    pool.close()
    pool.join()

print("âœ… Verwerking voltooid! Dataset opgeslagen als:", output_file)

from google.colab import files
files.download("actor_behavior_results.csv")

# Controleer of alle events binnen een case dezelfde outcome hebben
check = df.groupby("case_id")["case:outcome"].nunique()
print("Cases met meerdere outcomes:", (check > 1).sum())

!pip install matplotlib seaborn networkx

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx

# Dataset inlezen
df = pd.read_csv("actor_behavior_results.csv")

# ğŸ“Š 1. Histogram van gedragstypen
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="behavior", order=["C", "I", "HI", "HP", "HD"], palette="coolwarm")
plt.xlabel("Gedragstype")
plt.ylabel("Aantal")
plt.title("ğŸ“Š Distributie van Actor-Behavior Types")
plt.show()

# â³ 2. Tijd tussen events (delta_t) verdeling
plt.figure(figsize=(8, 5))
sns.histplot(df["delta_t"], bins=50, kde=True, color="purple")
plt.xlabel("Tijd tussen events (seconden)")
plt.ylabel("Aantal")
plt.title("â³ Verdeling van Tijd tussen Events")
plt.xlim(0, df["delta_t"].quantile(0.99))  # Zoom in op 99% van de data
plt.show()

# ğŸ”— 3. Netwerkgrafiek (optioneel, als dataset niet te groot is)
if len(df) < 5000:  # Vermijd grote netwerken
    G = nx.DiGraph()
    for _, row in df.iterrows():
        G.add_edge(row["actor_i"], row["actor_j"], label=row["behavior"])

    plt.figure(figsize=(10, 7))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(G, pos, node_size=500, node_color="lightblue", edge_color="gray", with_labels=True)
    plt.title("ğŸ”— Actor Interaction Network")
    plt.show()

import pandas as pd

# Dataset inlezen
df = pd.read_csv("actor_behavior_results.csv")

# Eerste 10 rijen tonen
print(df.head(10))

from IPython.display import display

display(df.head(10))

"""##Functie Hospital log"""

import gzip
import xml.etree.ElementTree as ET
import pandas as pd
import polars as pl
import multiprocessing as mp
from tqdm import tqdm

with gzip.open("Hospital_log.xes.gz", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

XES_NS = {'xes': 'http://www.xes-standard.org/'}
events = []
trace_tags = ['string', 'date', 'int', 'float', 'boolean', 'id']

for trace in root.findall('xes:trace', XES_NS):
    case_attrs = {}
    for tag in trace_tags:
        for attr in trace.findall(f'xes:{tag}', XES_NS):
            key = attr.attrib.get('key')
            value = attr.attrib.get('value')
            if key and value:
                case_attrs[key] = value
    case_id = case_attrs.get('concept:name', 'unknown_case')
    case_attrs['case_id'] = case_id
    for event in trace.findall('xes:event', XES_NS):
        e = case_attrs.copy()
        for attr in event:
            key = attr.attrib.get('key')
            value = attr.attrib.get('value')
            if key and value:
                e[key] = value
        events.append(e)

df = pd.DataFrame(events)

# =========================
# 2. Opschonen & timestamp correctie
# =========================

df = df.rename(columns={
    "concept:name": "event",
    "time:timestamp": "timestamp",
    "org:group": "actor"
})
df["timestamp"] = pd.to_datetime(df["timestamp"], errors='coerce')
df["actor"] = df["actor"].fillna("Unknown")

# =========================
# 3. KPI: operatie yes/no per case
# =========================
# KPI 'operatie' (based on keywords)
df['event_lower'] = df['event'].astype(str).str.lower()
operatie_keywords = ['operatie', 'resectie', 'excisie', 'extirpatie']
df['heeft_operatie'] = df['event_lower'].apply(lambda x: any(k in x for k in operatie_keywords))

# KPI per case
kpi_df = df.groupby("case_id")["heeft_operatie"].max().astype(bool).to_frame(name="operation_performed")
# Ensure case_id is a string in both dataframes
df["case_id"] = df["case_id"].astype(str)
kpi_df.index = kpi_df.index.astype(str)

# Add KPI
df = df.merge(kpi_df, on="case_id", how="left")

df.head(50)

df.to_csv("Hospital_log_converted.csv", index=False)

chunk_size = 100_000
num_workers = max(mp.cpu_count() - 1, 1)
output_file = "hospital_behavior_results.csv"

with open(output_file, "w") as f:
    f.write("case_id,event_i,timestamp_i,event_j,timestamp_j,actor_i,actor_j,behavior,delta_t,operation_performed\n")

# =========================
# 4. Gedrag classificatie via edges
# =========================

def classify_actor_behavior(df_dict):
    classified_edges = []
    actor_events = {}
    df = pl.DataFrame(df_dict)
    df = df.sort(["case_id", "timestamp"])

    for i in range(len(df) - 1):
        e_i, e_j = df.row(i, named=True), df.row(i + 1, named=True)
        if e_i["case_id"] != e_j["case_id"]:
            continue
        r_i, r_j = e_i["actor"], e_j["actor"]
        t_i, t_j = e_i["timestamp"], e_j["timestamp"]
        delta_t = (t_j - t_i).total_seconds()

        if r_i == r_j:
            concurrent = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_i, []))
            behavior = "I" if concurrent else "C"
        else:
            concurrent = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_j, []))
            behavior = "HI" if not concurrent else "HB"

        operatie_flag = e_i.get("operation_performed", "")

        classified_edges.append((
            e_i["case_id"], e_i["event"], t_i, e_j["event"], t_j,
            r_i, r_j, behavior, delta_t, operatie_flag
        ))

        actor_events.setdefault(r_i, []).append({"timestamp": t_i})
        actor_events.setdefault(r_j, []).append({"timestamp": t_j})

    return classified_edges

# =========================
# 5. Run verwerking
# =========================

def process_chunk(chunk):
    chunk = chunk.astype(str)  # fix mixed types
    chunk = pl.DataFrame(chunk)
    chunk = chunk.with_columns(pl.col("timestamp").str.to_datetime(strict=False))
    results = classify_actor_behavior(chunk.to_dict(as_series=False))

    with open(output_file, "a") as f:
        for row in results:
            f.write(",".join(map(str, row)) + "\n")
    return len(results)

with mp.Pool(num_workers) as pool:
    total_rows = sum(1 for _ in open("Hospital_log_converted.csv")) - 1
    with tqdm(total=total_rows, unit="rows") as pbar:
        for _ in pool.imap_unordered(process_chunk, pd.read_csv("Hospital_log_converted.csv", chunksize=chunk_size)):
            pbar.update(chunk_size)
    pool.close()
    pool.join()

print("âœ… Verwerking klaar. Resultaat in: hospital_behavior_results.csv")

from google.colab import files
files.download("hospital_behavior_results.csv")

# ğŸ“¦ Installatie (indien nodig)
!pip install matplotlib seaborn networkx

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx

# ğŸ“¥ Geclassificeerde dataset inlezen
valid_lines = []
with open("hospital_behavior_results.csv", "r") as f:
    for line in f:
        if len(line.strip().split(",")) == 9:
            valid_lines.append(line)

with open("hospital_behavior_results_clean.csv", "w") as f:
    f.writelines(valid_lines)

# Daarna veilig inlezen
df = pd.read_csv("hospital_behavior_results_clean.csv")

# ğŸ“Š 1. Histogram van gedragstypen
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="behavior", order=["C", "I", "HI", "HB"], palette="coolwarm")
plt.xlabel("Gedragstype")
plt.ylabel("Aantal")
plt.title("ğŸ“Š Distributie van Actor-Behavior Types (Hospital Log)")
plt.show()

# â³ 2. Verdeling van tijdsverschillen (delta_t)
plt.figure(figsize=(8, 5))
sns.histplot(df["delta_t"], bins=50, kde=True, color="teal")
plt.xlabel("Tijd tussen events (seconden)")
plt.ylabel("Aantal")
plt.title("â³ Verdeling van Tijdsverschil tussen Activiteiten")
plt.xlim(0, df["delta_t"].quantile(0.99))  # Zoom op 99% van data
plt.show()

# ğŸ”— 3. Netwerk van interacties tussen afdelingen
if len(df) < 5000:  # Voor kleinere datasets
    G = nx.DiGraph()
    for _, row in df.iterrows():
        G.add_edge(row["actor_i"], row["actor_j"], label=row["behavior"])

    plt.figure(figsize=(10, 7))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(G, pos, node_size=600, node_color="lightblue", edge_color="gray", with_labels=True)
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
    plt.title("ğŸ”— Interactienetwerk tussen Afdelingen")
    plt.show()

"""##Functie BPIC2012"""

import pandas as pd

# Preprocessing
df["time:timestamp"] = pd.to_datetime(df["time:timestamp"], errors="coerce", utc=True)
df["concept:name"] = df["concept:name"].astype(str).str.upper()

# Define and filter offer-related events
offer_events = ["O_ACCEPTED", "O_CANCELLED", "O_REFUSED"]
offers = df[df["concept:name"].isin(offer_events)].copy()
if offers.empty:
    print(df["concept:name"].dropna().unique()[:20])
else:
    # Retain last offer per case
    offers_sorted = offers.sort_values("time:timestamp")
    last_offer = offers_sorted.groupby("case:concept:name").tail(1)
    outcome_map = {
        "O_ACCEPTED": "Accepted",  "O_CANCELLED": "Canceled",  "O_REFUSED": "Refused" }
    last_offer["case:outcome"] = last_offer["concept:name"].map(outcome_map)

    # Merge
    df = df.merge(
        last_offer[["case:concept:name", "case:outcome"]],
        on="case:concept:name",
        how="left"
    )

    df = df[df["case:outcome"].notna()].copy()

    # Binary flags
    df["case_accepted"] = (df["case:outcome"] == "Accepted").astype(int)
    df["case_refused"] = (df["case:outcome"] == "Refused").astype(int)
    df["case_canceled"] = (df["case:outcome"] == "Canceled").astype(int)

    # Resultaat tonen
    print("âœ… Voorbeeldresultaat met uitkomst:")
    print(df[["case:concept:name", "concept:name", "case:outcome"]].head())

# Toon alle unieke waarden die met "O_" beginnen (case-insensitive)
o_concepts = df["concept:name"].dropna().unique()
o_concepts = [c for c in o_concepts if isinstance(c, str) and c.upper().startswith("O_")]
print("Aantal concept:name waarden die met 'O_' beginnen:", len(o_concepts))
print("Voorbeelden:", o_concepts[:10])

print("Unieke concept:name waarden:")
print(sorted(df["concept:name"].dropna().unique()))

print("Kolommen in df:")
print(df.columns)
print("\nEerste 5 rijen:")
print(df.head())

print(df["case:outcome"].value_counts())
print(df[["case_accepted", "case_refused", "case_canceled"]].sum())

# Selecteer relevante kolommen
df = df[["case:concept:name", "concept:name", "org:resource", "time:timestamp", "case:outcome","case_accepted", "case_canceled", "case_refused"]]
df.columns = ["case_id", "event", "actor", "timestamp", "case:outcome","case_accepted", "case_canceled", "case_refused"]
df["timestamp"] = pd.to_datetime(df["timestamp"])

# Opslaan als CSV
df.to_csv("BPIC12.csv", index=False)

from google.colab import files
files.download("BPIC12.csv")

print("ğŸ§ª Kolommen & types:")
print(df.dtypes)

print("\nğŸ” Aantal missende waarden per kolom:")
print(df[["case_id", "timestamp", "actor", "event"]].isna().sum())

print("\nğŸ§¾ Voorbeeld (eerste 3 rijen):")
print(df[["case_id", "timestamp", "actor", "event"]].head())

def classify_actor_behavior(df):
    """Bepaalt de actor-gedrag classificatie"""
    classified_edges = []
    actor_events = {}  # Snelle dictionary lookup

    df = df.sort("case_id", "timestamp")  # Sorteer case-logs op tijd

    for i in range(len(df) - 1):
        e_i, e_j = df[i], df[i + 1]

        # Controleer of de twee events bij dezelfde case horen
        if e_i["case_id"] != e_j["case_id"]:
            continue  # Als ze niet in dezelfde case zitten, gaan we verder

        # Actoren en tijdstippen ophalen
        r_i, r_j = e_i["actor"], e_j["actor"]
        t_i, t_j = e_i["timestamp"], e_j["timestamp"]
        event_i , event_j= e_i["event"], e_j["event"]
        delta_t = (t_j - t_i).total_seconds()  # Tijdverschil in seconden

        # Zelfde actor â†’ Continuation of Interruption
        if r_i == r_j:
            concurrent_cases = df[ (df["actor"] == r_i) & (df["timestamp"] > t_i) & (df["timestamp"] < t_j)] #event uit de dataframe waarvoor de actor r_i (=r_j is) en het tijdstip tussen ti en tj ligt
#Beslissing:
#Als concurrent_cases NIET leeg is â†’ Interruption (I).
#Als concurrent_cases wel leeg is â†’ Continuation (C).
            behavior = "I" if concurrent_cases else "C"
        else:
            # Andere actor â†’ Handover
            concurrent_cases = df[ (df["actor"] == r_j) & (df["timestamp"] > t_i) & (df["timestamp"] < t_j)] #event uit de dataframe waarvoor de actor r_j en het tijdstip tussen ti en tj ligt

            if not concurrent_cases:
                behavior = "HI"   # Handover Idle â†’ geen activiteit
            else:
                behavior = "HB"


        # Voeg resultaat toe
        classified_edges.append((e_i["case_id"], e_i["event"], t_i, e_j["event"], t_j, r_i, r_j, behavior, delta_t, e_i["Accepted"], e_j["Accepted"]))


        # Voeg actor events toe, maar bewaar alleen relevante cases
        actor_events.setdefault(r_i, []).append({"timestamp_i": t_i, "case_id": e_i["case_id"]})
        actor_events.setdefault(r_j, []).append({"timestamp_j": t_j, "case_id": e_j["case_id"]})

    return classified_edges

import polars as pl
import pandas as pd
import multiprocessing as mp
from tqdm import tqdm

input_file = "BPIC12.csv"
output_file = "bpic2012_behavior_results.csv"
chunk_size = 100000
num_workers = max(mp.cpu_count() - 1, 1)

# ğŸ“ Header schrijven
with open(output_file, "w") as f:
    f.write("case_id,event_i,timestamp_i,event_j,timestamp_j,actor_i,actor_j,behavior,delta_t,case_outcome,case_accepted,case_refused,case_canceled\n")

def classify_actor_behavior(df_dict):
    classified_edges = []
    actor_events = {}
    df = pl.DataFrame(df_dict)

    # Drop nulls before sorting!
    df = df.drop_nulls(subset=["case_id", "timestamp"])

    df = df.sort(["case_id", "timestamp"])

    for i in range(len(df) - 1):
        e_i, e_j = df.row(i, named=True), df.row(i + 1, named=True)

        if e_i["case_id"] != e_j["case_id"]:
            continue

        r_i, r_j = e_i["actor"], e_j["actor"]
        t_i, t_j = e_i["timestamp"], e_j["timestamp"]
        delta_t = (t_j - t_i).total_seconds()

        if r_i == r_j:
            concurrent_cases = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_i, []))
            behavior = "I" if concurrent_cases else "C"
        else:
            concurrent_cases = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_j, []))
            behavior = "HI" if not concurrent_cases else "HB"

        classified_edges.append((
            e_i["case_id"], e_i["event"], t_i, e_j["event"], t_j,
            r_i, r_j, behavior, delta_t,
            e_i.get("case:outcome"),
            e_i.get("case_accepted"), e_i.get("case_refused"), e_i.get("case_canceled")
        ))

        actor_events.setdefault(r_i, []).append({"timestamp": t_i, "case_id": e_i["case_id"]})
        actor_events.setdefault(r_j, []).append({"timestamp": t_j, "case_id": e_j["case_id"]})

    return classified_edges

def process_chunk(chunk):
    # Fix kolomnamen en parse timestamps
    chunk["timestamp"] = pd.to_datetime(chunk["timestamp"], errors="coerce")
    chunk = chunk.dropna(subset=["timestamp", "case_id"])

    if chunk.empty:
        return 0

    df_dict = chunk.to_dict(orient="list")
    results = classify_actor_behavior(df_dict)

    with open(output_file, "a") as f:
        for row in results:
            f.write(",".join(map(str, row)) + "\n")

    return len(results)

# ğŸš€ Multiprocessing run
with mp.Pool(num_workers) as pool:
    total_rows = sum(1 for _ in open(input_file)) - 1
    with tqdm(total=total_rows, unit="rows") as pbar:
        for _ in pool.imap_unordered(process_chunk, pd.read_csv(input_file, chunksize=chunk_size)):
            pbar.update(chunk_size)

print("âœ… Verwerking voltooid! Dataset opgeslagen als:", output_file)

from google.colab import files
files.download("bpic2012_behavior_results.csv")

# ğŸ“¦ Installatie (indien nodig)
!pip install matplotlib seaborn networkx

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx

# ğŸ“¥ Geclassificeerde dataset inlezen (zonder filter op kolom-aantal)
df = pd.read_csv("bpic2012_behavior_results.csv")

# ğŸ“Š 1. Histogram van gedragstypen
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="behavior", order=["C", "I", "HI", "HB"], palette="coolwarm")
plt.xlabel("Gedragstype")
plt.ylabel("Aantal")
plt.title("ğŸ“Š Distributie van Actor-Behavior Types (BPIC 2012)")
plt.show()

# â³ 2. Verdeling van tijdsverschillen (delta_t)
plt.figure(figsize=(8, 5))
sns.histplot(df["delta_t"], bins=50, kde=True, color="teal")
plt.xlabel("Tijd tussen events (seconden)")
plt.ylabel("Aantal")
plt.title("â³ Verdeling van Tijdsverschil tussen Activiteiten")
plt.xlim(0, df["delta_t"].quantile(0.99))  # Zoom op 99% van data
plt.show()

# ğŸ”— 3. Netwerk van interacties tussen afdelingen (alleen voor kleine datasets)
if len(df) < 5000:
    G = nx.DiGraph()
    for _, row in df.iterrows():
        G.add_edge(row["actor_i"], row["actor_j"], label=row["behavior"])

    plt.figure(figsize=(10, 7))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(G, pos, node_size=600, node_color="lightblue", edge_color="gray", with_labels=True)
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
    plt.title("ğŸ”— Interactienetwerk tussen Afdelingen")
    plt.show()

"""##Functie BPIC2019"""

import xml.etree.ElementTree as ET
import pandas as pd
import pm4py

# ğŸ“¥ 1. Lees de event log in met PM4PY
log = pm4py.read_xes("BPI_Challenge_2019.xes")
df = pm4py.convert_to_dataframe(log)

# ğŸ“‚ 2. Parse trace-level attribuut 'case:Goods Receipt' uit XML
with open("BPI_Challenge_2019.xes", 'rb') as f:
    tree = ET.parse(f)
    root = tree.getroot()

XES_NS = {'xes': 'http://www.xes-standard.org/'}
trace_data = []

for trace in root.findall("xes:trace", XES_NS):
    case_id = None
    goods_receipt = None

    for attr in trace:
        tag = attr.tag.split("}")[1]
        key = attr.get("key")
        value = attr.get("value")

        if key == "concept:name":
            case_id = value
        elif key == "case:Goods Receipt":
            goods_receipt = value

    if case_id is not None:
        trace_data.append((case_id, goods_receipt))

# Zet in DataFrame en hernoem kolom (geen case:-prefix)
trace_df = pd.DataFrame(trace_data, columns=["case_id", "Goods Receipt"])

# ğŸ”„ Merge trace-level info met events
df = df.rename(columns={"case:concept:name": "case_id"})
df = df.merge(trace_df, on="case_id", how="left")

# ğŸ§¹ Opschonen
df["timestamp"] = pd.to_datetime(df["time:timestamp"], errors='coerce')
df["org:resource"] = df["org:resource"].fillna("Unknown")
df = df[~df["org:resource"].str.lower().str.startswith("batch_")]

# âœ… Hernoem kolommen
df = df.rename(columns={
    "org:resource": "actor",
    "concept:name": "event"
})

# ğŸ” Zet 'true'/'false' naar echte booleans
df["Goods Receipt"] = df["Goods Receipt"].replace({"true": True, "false": False})

# ğŸ’¾ Export
df.to_csv("bpic2019_log.csv", index=False)
print("âœ… CSV opgeslagen als 'bpic2019_log.csv'")

df.head()

# ğŸ“¦ Vereiste packages
!pip install polars tqdm

import polars as pl
import pandas as pd
import multiprocessing as mp
from tqdm import tqdm

# ğŸ“‚ Configuratie
input_file = "bpic2019_log.csv"
output_file = "bpic2019_behavior_results.csv"
chunk_size = 100_000
num_workers = max(mp.cpu_count() - 1, 1)

# ğŸ“ Outputbestand initialiseren
with open(output_file, "w") as f:
    f.write("case_id,event_i,timestamp_i,event_j,timestamp_j,actor_i,actor_j,behavior,case:Goods Receipt, delta_t\n")

# ğŸ” Gedragscoderingsfunctie
def classify_actor_behavior(df_dict):
    classified_edges = []
    actor_events = {}
    df = pl.DataFrame(df_dict)
    df = df.sort(["case_id", "timestamp"])

    for i in range(len(df) - 1):
        e_i, e_j = df.row(i, named=True), df.row(i + 1, named=True)
        if e_i["case_id"] != e_j["case_id"]:
            continue

        r_i, r_j = e_i["actor"], e_j["actor"]
        t_i, t_j = e_i["timestamp"], e_j["timestamp"]
        delta_t = (t_j - t_i).total_seconds()

        if r_i == r_j:
            concurrent_cases = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_i, []))
            behavior = "I" if concurrent_cases else "C"
        else:
            concurrent_cases = any(t_i < e["timestamp"] < t_j for e in actor_events.get(r_j, []))
            behavior = "HI" if not concurrent_cases else ("HB")

        classified_edges.append((
            e_i["case_id"], e_i["event"], t_i, e_j["event"], t_j,
            r_i, r_j, behavior,e_i.get("case:Goods Receipt"), delta_t
        ))

        actor_events.setdefault(r_i, []).append({"timestamp": t_i, "case_id": e_i["case_id"]})
        actor_events.setdefault(r_j, []).append({"timestamp": t_j, "case_id": e_j["case_id"]})

    return classified_edges

# ğŸ”„ Verwerk chunks
def process_chunk(chunk):
    chunk = pl.DataFrame(chunk)
    chunk = chunk.with_columns(pl.col("timestamp").str.to_datetime(strict=False))
    results = classify_actor_behavior(chunk.to_dict(as_series=False))

    with open(output_file, "a") as f:
        for row in results:
            f.write(",".join(map(str, row)) + "\n")

    return len(results)

# ğŸš€ Uitvoeren met multiprocessing
with mp.Pool(num_workers) as pool:
    total_rows = sum(1 for _ in open(input_file)) - 1

    with tqdm(total=total_rows, unit="rows") as pbar:
        for _ in pool.imap_unordered(process_chunk, pd.read_csv(input_file, chunksize=chunk_size)):
            pbar.update(chunk_size)

    pool.close()
    pool.join()

print("âœ… Verwerking voltooid! Resultaat opgeslagen in:", output_file)

from google.colab import files
files.download("bpic2019_behavior_results.csv")

from google.colab import files
import pandas as pd
import pm4py

# Upload de dataset handmatig
uploaded = files.upload()

# 1. Laad de dataset
df = pd.read_csv("bpic2019_behavior_results (4).csv", parse_dates=["timestamp_i", "timestamp_j"])
df.head()

# ğŸ“¦ Installatie (indien nodig)
!pip install matplotlib seaborn networkx

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import networkx as nx

# Daarna veilig inlezen
df = pd.read_csv("bpic2019_behavior_results (4).csv")

# ğŸ“Š 1. Histogram van gedragstypen
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="behavior", order=["C", "I", "HI", "HB"], palette="coolwarm")
plt.xlabel("Gedragstype")
plt.ylabel("Aantal")
plt.title("ğŸ“Š Distributie van Actor-Behavior Types (Hospital Log)")
plt.show()

# â³ 2. Verdeling van tijdsverschillen (delta_t)
plt.figure(figsize=(8, 5))
sns.histplot(df["delta_t"], bins=50, kde=True, color="teal")
plt.xlabel("Tijd tussen events (seconden)")
plt.ylabel("Aantal")
plt.title("â³ Verdeling van Tijdsverschil tussen Activiteiten")
plt.xlim(0, df["delta_t"].quantile(0.99))  # Zoom op 99% van data
plt.show()

# ğŸ”— 3. Netwerk van interacties tussen afdelingen
if len(df) < 5000:  # Voor kleinere datasets
    G = nx.DiGraph()
    for _, row in df.iterrows():
        G.add_edge(row["actor_i"], row["actor_j"], label=row["behavior"])

    plt.figure(figsize=(10, 7))
    pos = nx.spring_layout(G, seed=42)
    nx.draw(G, pos, node_size=600, node_color="lightblue", edge_color="gray", with_labels=True)
    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
    plt.title("ğŸ”— Interactienetwerk tussen Afdelingen")
    plt.show()